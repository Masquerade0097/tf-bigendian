{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "ByteSwap-FrozenModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPp0V2gNwXc0"
      },
      "source": [
        "!pip3 install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpQMA6ZAJuty"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtXahsMuJut5"
      },
      "source": [
        "tf.__version__\n",
        "# !protoc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch04XzjsJuuL"
      },
      "source": [
        "# Download the Tensorflow model\n",
        "!wget 'http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz'\n",
        "!tar -xzf ssd_mobilenet_v1_coco_2018_01_28.tar.gz\n",
        "!rm ssd_mobilenet_v1_coco_2018_01_28.tar.gz\n",
        "%cd ssd_mobilenet_v1_coco_2018_01_28/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9NT8rr_kRlT"
      },
      "source": [
        "from tensorflow.core.framework import graph_pb2\n",
        "from tensorflow.python.client import session\n",
        "from tensorflow.python.platform import gfile\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import importer\n",
        "\n",
        "graph_def_proto = \"\"\n",
        "\n",
        "# Import the protobuf graph definition from the frozen graph and load it in Tensorflow\n",
        "with session.Session(graph=ops.Graph()) as sess:\n",
        "  with gfile.GFile(\"frozen_inference_graph.pb\", \"rb\") as f:\n",
        "    graph_def_proto = graph_pb2.GraphDef()\n",
        "    graph_def_proto.ParseFromString(f.read())\n",
        "    importer.import_graph_def(graph_def_proto)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7O-gWLn5ldA"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Byte swap the\n",
        "def swap16(x):\n",
        "  return np.frombuffer(x, dtype=np.uint16).byteswap().tobytes()\n",
        "\n",
        "def swap32(x):\n",
        "  return np.frombuffer(x, dtype=np.uint32).byteswap().tobytes()\n",
        "\n",
        "def swap64(x):\n",
        "  return np.frombuffer(x, dtype=np.uint64).byteswap().tobytes()\n",
        "\n",
        "def byte_swap_array(content, bytes_per_elem):\n",
        "    if bytes_per_elem == 1:\n",
        "      return content\n",
        "\n",
        "    elif bytes_per_elem == 2:\n",
        "      return swap16(content)\n",
        "\n",
        "    elif bytes_per_elem == 4:\n",
        "      return swap32(content)\n",
        "\n",
        "    elif bytes_per_elem == 8:\n",
        "      return swap64(content)\n",
        "\n",
        "    else:\n",
        "      return content"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYxdHsI7JuuU"
      },
      "source": [
        "set_allowed = set([tf.string, tf.qint8, tf.quint8, tf.bool, tf.int8, tf.uint8])\n",
        "set_16bits = set([tf.bfloat16, tf.half, tf.qint16, tf.quint16, tf.uint16, tf.int16])\n",
        "set_32bits = set([tf.float32, tf.int32, tf.qint32, tf.uint32])\n",
        "set_64bits = set([tf.int64, tf.uint64, tf.double])\n",
        "\n",
        "# Separate the tensors according to tf.dtype of their tensor_content\n",
        "def byte_swap_tensor(orig_tensor) -> bytes:\n",
        "\n",
        "    datatype = orig_tensor.dtype\n",
        "    content = orig_tensor.tensor_content\n",
        "    bytes_per_elem = 0\n",
        "\n",
        "    if datatype in set_allowed:\n",
        "      bytes_per_elem = 1\n",
        "    elif datatype in set_16bits:\n",
        "      bytes_per_elem = 2\n",
        "    elif datatype in set_32bits:\n",
        "      bytes_per_elem = 4\n",
        "    elif datatype in set_64bits:\n",
        "      bytes_per_elem = 8\n",
        "    elif datatype in set_complex64:\n",
        "      bytes_per_elem = 4\n",
        "    elif datatype in set_complex128:\n",
        "      bytes_per_elem = 8\n",
        "    else:\n",
        "      print(\"Byteswapping not supported for datatype\")\n",
        "      return content\n",
        "  \n",
        "    return byte_swap_array(content, bytes_per_elem)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRTxyRlSJuuX"
      },
      "source": [
        "# Iterate over the nodes of graphs in the top-level graph to identify all the tensors. The `tensor_content` field has the same byte order \n",
        "# of the machine it was created on (x86 machine with little-endian byte order).\n",
        "# So, we convert the byte order of 'tensor_content' from little-endian to big-endian which makes it possible for big-endian archs to load TF frozen models .\n",
        "for n in graph_def_proto.node:\n",
        "    # print(n.name)\n",
        "    for attr_name in n.attr:\n",
        "        attr_value = n.attr[attr_name]\n",
        "        if attr_value.HasField(\"tensor\"):   \n",
        "            tensor_value = attr_value.tensor\n",
        "            if len(tensor_value.tensor_content) > 0:\n",
        "              print(f\"       Before: {attr_name} => {tensor_value.tensor_content}\")\n",
        "              swapped_tensor_content = byte_swap_tensor(tensor_value)\n",
        "              tensor_value.tensor_content = swapped_tensor_content\n",
        "              print(f\"       After: {attr_name} => {tensor_value.tensor_content}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiHGj183Juug",
        "outputId": "ce2495bb-f876-48bb-a83b-2c3d20683b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "binary_saved_model_proto = graph_def_proto.SerializeToString()\n",
        "len(binary_saved_model_proto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29110448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6z-ZbtOJuum"
      },
      "source": [
        "# Saving the rewitten frozen graph\n",
        "with open(\"swapped_frozen_inference_graph.pb\", \"wb\") as f:\n",
        "    f.write(binary_saved_model_proto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJOkAKlFd9qW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}